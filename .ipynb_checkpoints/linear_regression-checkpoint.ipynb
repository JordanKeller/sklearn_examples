{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_output) = load_boston(return_X_y = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loaded Boston housing data consisting of 506 training samples. The input is a collection of thirteen home features (see https://scikit-learn.org/stable/datasets/index.html#boston-dataset for details), and the output is a home price (in thousands of dollars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "       6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "       4.980e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_input.mean(axis = 0)\n",
    "std = train_input.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.61352356e+00, 1.13636364e+01, 1.11367787e+01, 6.91699605e-02,\n",
       "       5.54695059e-01, 6.28463439e+00, 6.85749012e+01, 3.79504269e+00,\n",
       "       9.54940711e+00, 4.08237154e+02, 1.84555336e+01, 3.56674032e+02,\n",
       "       1.26530632e+01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.59304135e+00, 2.32993957e+01, 6.85357058e+00, 2.53742935e-01,\n",
       "       1.15763115e-01, 7.01922514e-01, 2.81210326e+01, 2.10362836e+00,\n",
       "       8.69865112e+00, 1.68370495e+02, 2.16280519e+00, 9.12046075e+01,\n",
       "       7.13400164e+00])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input -= mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.05245508e-15,  2.07616094e-14, -2.80039496e-14, -1.18975975e-16,\n",
       "        2.64171645e-16, -8.14105042e-15, -3.55271368e-14,  1.72018745e-15,\n",
       "        4.68312258e-15,  3.37016317e-13, -2.34858246e-14,  7.63229620e-13,\n",
       "       -3.56500073e-15])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centered and scaled each of the input features via subtraction of the mean and (elementwise) division of the standard deviation.  Notice the use of broadcasting.\n",
    "\n",
    "We could equally well use the sklearn.preprocessing.StandardScaler helper object to perform this feature normalization; see e.g. logistic_regression.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'normalize': False}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406426641094095"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.score(train_input, train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearRegression learns the feature weights by means of the closed form normal equations.  As such, there are relatively few parameters governing the object.  Note the 'fit_intercept' variable, True when we wish to include a bias term (roughly, Y = \\Theta . X + B), False otherwise (Y = \\Theta . X).  Note also the 'normalize' parameter, True if we wish to normalize our feature data (done earlier in this notebook), False otherwise.  Defaults are given above: fit_intercept = True, normalize = False.\n",
    "\n",
    "Performance of the model is indicated via the .score() method, which returns the R^2 value of an (output, input) collection of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001,\n",
       " 'average': False,\n",
       " 'early_stopping': False,\n",
       " 'epsilon': 0.1,\n",
       " 'eta0': 0.01,\n",
       " 'fit_intercept': True,\n",
       " 'l1_ratio': 0.15,\n",
       " 'learning_rate': 'invscaling',\n",
       " 'loss': 'squared_loss',\n",
       " 'max_iter': 1000,\n",
       " 'n_iter_no_change': 5,\n",
       " 'penalty': 'l2',\n",
       " 'power_t': 0.25,\n",
       " 'random_state': None,\n",
       " 'shuffle': True,\n",
       " 'tol': 0.001,\n",
       " 'validation_fraction': 0.1,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(alpha=0.0001, average=False, early_stopping=False, epsilon=0.1,\n",
       "             eta0=0.01, fit_intercept=True, l1_ratio=0.15,\n",
       "             learning_rate='invscaling', loss='squared_loss', max_iter=1000,\n",
       "             n_iter_no_change=5, penalty='l2', power_t=0.25, random_state=None,\n",
       "             shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "             warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_input, train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7396987369567145"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.score(train_input, train_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDRegressor applies stochastic gradient descent to a given loss function.  Note its large number of parameters relative to the earlier LinearRegression (including the earlier 'fit_intercept' parameter).  \n",
    "\n",
    "The optimization objective is specified via the 'loss' parameter, providing the loss function, the 'penalty' parameter, providing regularization terms, and the 'alpha' parameter, providing the regularization coefficient.  By default, we have a standard L^2 loss function (loss = 'squared_loss') with L^2 regularization (penalty = 'l2') and regularization coefficient alpha = .0001 (i.e., ridge regression).  One could also consider L^1 regularization (lasso regression) or a combination of L^2 and L^1 regularization (via the 'l1_ratio' parameter).\n",
    "\n",
    "Next, we have parameters governing the stochastic gradient descent algorithm.  In particular, we specify a learning rate scheme (static or adaptive) using various parameters ('learning_rate', 'eta0', 'power_t') and control the total number of iterations over the data set via the 'max_iter' parameter.  Early stopping criterion can also be specified (via e.g. 'early_stopping', 'n_iter_no_change', 'validation_fraction', 'tol' parameters).\n",
    "\n",
    "Notice that there is no specification of batch size in the parameter list.  In fact, one must use the .partial_fit() method to train over batches of data.  In the absence of this method, the SGDRegressor object performs ordinary gradient descent.  Useful parameters for batch gradient descent include 'shuffle', 'random_state' and 'warm_start'.\n",
    "\n",
    "As with the LinearRegression object, performance of the model is indicated via the .score() method, which returns the R^2 value of an (output, input) collection of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:venv] *",
   "language": "python",
   "name": "conda-env-venv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
